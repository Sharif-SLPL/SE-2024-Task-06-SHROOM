{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRa1OQl6S2jS"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import time\n",
        "import pandas as pd\n",
        "import json"
      ],
      "id": "BRa1OQl6S2jS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iR-NnrNRVKYN",
        "outputId": "1872ced1-3a79-4164-d29c-fe74699ae090"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Collecting jsonlines\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines) (23.1.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.11.17)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-4.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown jsonlines"
      ],
      "id": "iR-NnrNRVKYN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAVAuzbJTVjS",
        "outputId": "d3bc0679-0871-4a0a-ded9-c1cee2b0b585"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieving folder list\n",
            "Processing file 1Qme_usldDMvuUv9vIdWFbesCGJvVAX2_ test_GPT_data_DM.json\n",
            "Processing file 1bBPcfXxPIL31-ApqccOShyNi_1A2d9fh test_GPT_data_MT.json\n",
            "Processing file 19iLZ_rAPQjQ6anIbrYAbmOidumCdXFmj test_GPT_data_PG.json\n",
            "Processing file 1LfbKqgoq0ewR0pxt0f4QS0HXsM9hxSVw train.model-agnostic.json\n",
            "Processing file 1B7xfizWEgdn9pzA5vn4nwdyfDVF5gCoK train.model-aware.json\n",
            "Processing file 1DbPnEes5bpYXCc9l3rlTKyVPd_4xLcFi trial-v1.json\n",
            "Processing file 17OeAjz1lBXV2p48RL8zueHh8uR82tn_6 val.model-agnostic.json\n",
            "Processing file 1Q-clSoOuDBl0IbVmirav1LEE2kIN3LJh val.model-aware.v2.json\n",
            "Retrieving folder list completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Qme_usldDMvuUv9vIdWFbesCGJvVAX2_\n",
            "To: /content/SE2024_Task06_Group_01/test_GPT_data_DM.json\n",
            "100% 2.89k/2.89k [00:00<00:00, 9.17MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1bBPcfXxPIL31-ApqccOShyNi_1A2d9fh\n",
            "To: /content/SE2024_Task06_Group_01/test_GPT_data_MT.json\n",
            "100% 4.50k/4.50k [00:00<00:00, 10.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19iLZ_rAPQjQ6anIbrYAbmOidumCdXFmj\n",
            "To: /content/SE2024_Task06_Group_01/test_GPT_data_PG.json\n",
            "100% 2.29k/2.29k [00:00<00:00, 10.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1LfbKqgoq0ewR0pxt0f4QS0HXsM9hxSVw\n",
            "To: /content/SE2024_Task06_Group_01/train.model-agnostic.json\n",
            "100% 8.81M/8.81M [00:00<00:00, 50.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1B7xfizWEgdn9pzA5vn4nwdyfDVF5gCoK\n",
            "To: /content/SE2024_Task06_Group_01/train.model-aware.json\n",
            "100% 10.2M/10.2M [00:00<00:00, 128MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1DbPnEes5bpYXCc9l3rlTKyVPd_4xLcFi\n",
            "To: /content/SE2024_Task06_Group_01/trial-v1.json\n",
            "100% 34.5k/34.5k [00:00<00:00, 49.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=17OeAjz1lBXV2p48RL8zueHh8uR82tn_6\n",
            "To: /content/SE2024_Task06_Group_01/val.model-agnostic.json\n",
            "100% 265k/265k [00:00<00:00, 87.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Q-clSoOuDBl0IbVmirav1LEE2kIN3LJh\n",
            "To: /content/SE2024_Task06_Group_01/val.model-aware.v2.json\n",
            "100% 302k/302k [00:00<00:00, 88.3MB/s]\n",
            "Download completed\n"
          ]
        }
      ],
      "source": [
        "# Get data from drive\n",
        "!gdown --folder https://drive.google.com/drive/folders/1t_-L8928Oha89idlB8zFT5oP9yM63sDI?usp=sharing"
      ],
      "id": "oAVAuzbJTVjS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bQK6d7PR9Tk"
      },
      "outputs": [],
      "source": [
        "directory='/content/SE2024_Task06_Group_01'"
      ],
      "id": "9bQK6d7PR9Tk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c4a064f-e732-45ed-91b2-ac51d20f66c0"
      },
      "outputs": [],
      "source": [
        "api_key = \"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\"\n",
        "prompt=['I used a language model to define certain word in a sentence. Here are the hypothesis that the model generated, target which is the true definition of the word and sentence which is the sentence on which the word we want to define is:\\n hypothesis: <hyp> \\n target: <tgt> \\nsentence: <src> \\n Can you determine whether the language model has hallucinated or not? answer it with just a number between 0 and 1 where 1 means it is a complete hallucination and 0 means it is not. don\\'t write anything else.',\n",
        "        'DM_model',\n",
        "        'PG_model']\n",
        "\n",
        "# prompt for MT model\n",
        "prompt[0]='''Here is the translation of a sentence using a language model. Sentence is the one that was translated and target is a desired translation.\n",
        "Can you determine whether the language model has hallucinated or not? answer it with a number between 0 to 5, where 0 means it is not hallucination and 5 means it is a complete hallucination.\n",
        "\n",
        "Here are examples for different ratings:\n",
        "hypothesis: The moon's surface is made of rock and dust.\n",
        "target: The surface of the Moon is made of rocks and dust. The outer layer of the Moon is called the crust.\n",
        "sentence: Kor due olos kod kite gi buru. Oko mar due iluongoni kama tek.\n",
        "hallucination rating: 0\n",
        "\n",
        "hypothesis: Another biorhythm-based alternative involves drinking plenty of fluid (such as water or tea, known as water-driven fluid) before bedtime, which will force one to stand up to urinate.\n",
        "target: Other biorhythm-based options involve drinking lots of fluid (particularly water or tea, a known diuretic) prior to sleep, forcing one to get up to urinate.\n",
        "sentence: Andre biorytme-baserte alternativ involverer \\u00e5 drikke mykje v\\u00e6ske (s\\u00e6rleg vatn eller te, kjende vatndrivande v\\u00e6sker) f\\u00f8r s\\u00f8vn, som vil tvinge ein til \\u00e5 st\\u00e5 opp for \\u00e5 urinere.\n",
        "hallucination rating: 1\n",
        "\n",
        "hypothesis: Like a television switch, the controller uses two transmitters placed next to the user's television to transmit its position in a triangular space.\n",
        "target: Resembling a television remote, the controller uses two sensors placed near the user's television to triangulate its position in three-dimensional space.\n",
        "sentence: Cos\\u00fail le cianrialt\\u00e1n teilif\\u00edse, \\u00fas\\u00e1ideann an rialaitheoir dh\\u00e1 bhraiteoir a chuirtear in aice le teilif\\u00eds an \\u00fas\\u00e1ideora chun a shu\\u00edomh a thriant\\u00e1n\\u00fa i sp\\u00e1s tr\\u00edthoiseach.\n",
        "hallucination rating: 2\n",
        "\n",
        "hypothesis: The Director of Public Notes, Kier Starmer QC, came with the announcement of the notes by both Huhne and Pryce.\n",
        "target: The Director of Public Prosecutions, Kier Starmer QC, gave a statement this morning announcing the prosecution of both Huhne and Pryce.\n",
        "sentence: Direkt\\u00f8ren for offentlege p\\u00e5talar, Kier Starmer QC, kom med ei ytring som kunngjorde p\\u00e5tale av b\\u00e5de Huhne og Pryce.\n",
        "hallucination rating: 3\n",
        "\n",
        "hypothesis: In addition to the events held on Buddha Day, Carpanedo participated in the centuries-long events of the individual road race championships.\n",
        "target: Beyond Wednesday's event, Carpanedo competed in two individual races at the Championships.\n",
        "sentence: Tiosan saking acara san\\u00e9 mamargi ring rahina Buda, Carpanedo san\\u00e9 nyarengin pacentokan ring kalih acara kejuaraan balapan individu\n",
        "hallucination rating: 4\n",
        "\n",
        "hypothesis: Moldova is a country where racial and ethnic violence is rampant.\n",
        "target: Moldova is a multi-ethnic republic that has suffered from ethnic conflict.\n",
        "sentence: Moldova y\\u025b \\u0254man a ekura mmusuakuw dodow no ara na mmusuakuw nt\\u0254kwaw ak\\u0254 so w\\u0254 h\\u0254 pa ara.\n",
        "hallucination rating: 5\n",
        "\n",
        "Here is the example that you should rate:\n",
        "hypothesis: <hyp>\n",
        "target: <tgt>\n",
        "sentence: <src>\n",
        "Answer it with just the rating number and nothing else.\n",
        "'''\n",
        "\n",
        "# prompt for DM model\n",
        "prompt[1]='''\n",
        "I used a language model for a Definition Modeling task where model should give us definition of the word which is specified between <define> tags. I'm going to give you model output which is hypothesis, its input which is sentence and desired output which is target. Your job is to give it a rating between 0 to 5 where 0 means our model didn't hallucinate at all and 5 means its output was a complete hallucination.\n",
        "Here are examples for each rating:\n",
        "\n",
        "for rate 0:\n",
        "hypothesis: To react too much .\n",
        "sentence: Please try not to <define> overreact </define> if she drives badly when she is first learning .\n",
        "target: To react too much or too intensely .\n",
        "\n",
        "for rate 1:\n",
        "hypothesis: Having four dimensions .\n",
        "sentence: We live in a <define> four-dimensional </define> universe .\n",
        "target: Relating to the four dimensions of space-time ( three spatial dimensions with the addition of time as the fourth )\n",
        "\n",
        "for rate 2:\n",
        "hypothesis: Not yet made , or used , by a particular person or process .\n",
        "sentence: <define> Virgin </define> clay , i.e . clay that has never been fired .\n",
        "target: Not yet cultivated , explored , or exploited by humans or humans of certain civilizations .\n",
        "\n",
        "for rate 3:\n",
        "hypothesis: The process of spoiling ; the state of being spoilt .\n",
        "sentence: To prevent <define> spoilage </define> , store in a cool , dry place .\n",
        "target: The process of spoiling .\n",
        "\n",
        "for rate 4:\n",
        "hypothesis: A sum of money , especially one that is not a fixed amount .\n",
        "sentence: Several major <define> funds </define> were declared insolvent recently .\n",
        "target: A money-management operation , such as a mutual fund .\n",
        "\n",
        "for rate 5:\n",
        "hypothesis: To suffocate .\n",
        "sentence: Having <define> divested </define> the child he kissed her gently and gave her a little pat to make her stand off .\n",
        "target: To undress .\n",
        "\n",
        "Now what is the rate of example below. Only write the number.\n",
        "hypothesis: <hyp>\n",
        "sentence: <src>\n",
        "target: <tgt>\n",
        "'''\n",
        "\n",
        "# prompt for PG model\n",
        "prompt[2]='''\n",
        "Here is the output of a model in paraphrase generating task and its input.\n",
        "output: <hyp>\n",
        "input: <src>\n",
        "Can you determine whether the language model has hallucinated or not? answer it with a number between 0 and 5 where 5 means it is a complete hallucination and 0 means it is not.\n",
        "Here are some examples:\n",
        "for rate 0:\n",
        "output: I think I'm going to be the one who's going to need it.\n",
        "input: I think I'm going to need it.\n",
        "\n",
        "for rate 1:\n",
        "output: I'm a very good at keeping a secret.\n",
        "input: I'm good at keeping secrets.\n",
        "\n",
        "for rate 2:\n",
        "output: moral values are the basis of the European Union's economic union.\n",
        "input: The European Union is first and foremost an economic union based on moral values.\n",
        "\n",
        "for rate 3:\n",
        "output: I'll see you in a couple of days, if you're going to be able to get a drink.\n",
        "input: I'll see you in a couple of days.\n",
        "\n",
        "for rate 4:\n",
        "output: There's something wrong with me, man.\n",
        "input: Something's not right.\n",
        "\n",
        "for rate 5:\n",
        "output: I didn't sign up for this, and I didn't want to be in the car.\n",
        "input: I didn't sign up for this.\n",
        "\n",
        "Write only the number and nothing else.\n",
        "'''\n"
      ],
      "id": "9c4a064f-e732-45ed-91b2-ac51d20f66c0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSy5W5NFdf0v",
        "outputId": "21ef2a98-4f18-4c54-a18e-f17014007630"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2999"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(prompt[0])"
      ],
      "id": "kSy5W5NFdf0v"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNXtLY0QdiIx",
        "outputId": "84bb6352-c841-4aa9-911d-f5a777611b30"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1855"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(prompt[1])"
      ],
      "id": "dNXtLY0QdiIx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLl6BU8qdjWn",
        "outputId": "1ae7b24c-2ffa-4a59-b127-5d1649b521d5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1076"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(prompt[2])"
      ],
      "id": "QLl6BU8qdjWn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cde68ad8-c689-4947-8c7e-33b260bf1974"
      },
      "outputs": [],
      "source": [
        "\n",
        "def chat_with_gpt(prompt, api_key):\n",
        "    endpoint = \"https://api.openai.com/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {api_key}\"\n",
        "    }\n",
        "    data = {\n",
        "        \"model\": \"gpt-3.5-turbo\",\n",
        "        \"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                     {\"role\": \"user\", \"content\": prompt}]\n",
        "    }\n",
        "\n",
        "    response = requests.post(endpoint, headers=headers, json=data)\n",
        "    response.raise_for_status()  # Check for errors\n",
        "    time.sleep(18)\n",
        "    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n"
      ],
      "id": "cde68ad8-c689-4947-8c7e-33b260bf1974"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91759904-c800-4c5b-9055-e00f6ac5c120"
      },
      "outputs": [],
      "source": [
        "def MT_model_req(data, prompt):\n",
        "    gpt_prompt = prompt[0].replace(\"<hyp>\",data['hyp'])\n",
        "    gpt_prompt = gpt_prompt.replace(\"<tgt>\",data['tgt'])\n",
        "    gpt_prompt = gpt_prompt.replace(\"<src>\",data['src'])\n",
        "\n",
        "    return chat_with_gpt(gpt_prompt,api_key)"
      ],
      "id": "91759904-c800-4c5b-9055-e00f6ac5c120"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dfc340e"
      },
      "outputs": [],
      "source": [
        "def DM_model_req(data, prompt):\n",
        "\n",
        "    gpt_prompt = prompt[1].replace(\"<hyp>\",data['hyp'])\n",
        "    gpt_prompt = gpt_prompt.replace(\"<tgt>\",data['tgt'])\n",
        "    gpt_prompt = gpt_prompt.replace(\"<src>\",data['src'])\n",
        "\n",
        "    return chat_with_gpt(gpt_prompt,api_key)"
      ],
      "id": "9dfc340e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48851991"
      },
      "outputs": [],
      "source": [
        "def PG_model_req(data, prompt):\n",
        "    gpt_prompt = prompt[2].replace(\"<hyp>\",data['hyp'])\n",
        "    gpt_prompt = gpt_prompt.replace(\"<src>\",data['src'])\n",
        "\n",
        "    return chat_with_gpt(gpt_prompt,api_key)"
      ],
      "id": "48851991"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "341529d0-9e69-4b36-887e-10ae1ff313b0"
      },
      "outputs": [],
      "source": [
        "def data_enc(data,prompt):\n",
        "    if data['task']==\"MT\" :\n",
        "        result = MT_model_req(data,prompt)\n",
        "    elif data['task']==\"DM\" :\n",
        "        result = DM_model_req(data,prompt)\n",
        "    elif data['task']==\"PG\" :\n",
        "        result = PG_model_req(data,prompt)\n",
        "    else:result=\"input data not cruct in column task\"\n",
        "    return result"
      ],
      "id": "341529d0-9e69-4b36-887e-10ae1ff313b0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17062c1b"
      },
      "outputs": [],
      "source": [
        "def set_label(data,prompt,n,):\n",
        "    data['GPT_label'] = None\n",
        "    for i, idx in enumerate(data.index):\n",
        "        if i >= n:\n",
        "            break\n",
        "        ds=data.loc[idx].copy()\n",
        "        data.loc[idx, 'GPT_label']=data_enc(ds,prompt)\n",
        "    return data\n"
      ],
      "id": "17062c1b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4da5d18"
      },
      "outputs": [],
      "source": [
        "with open(directory+\"/train.model-agnostic.json\", \"r\") as f:\n",
        "    train_agnostic = pd.DataFrame(json.loads(f.read()))\n",
        "with open(directory+\"/train.model-aware.json\", \"r\") as f:\n",
        "    train_aware = pd.DataFrame(json.loads(f.read()))\n",
        "with open(directory+\"/val.model-agnostic.json\", \"r\") as f:\n",
        "    val_agnostic = pd.DataFrame(json.loads(f.read()))\n",
        "with open(directory+\"/val.model-aware.v2.json\", \"r\") as f:\n",
        "    val_aware = pd.DataFrame(json.loads(f.read()))\n",
        "with open(directory+\"/trial-v1.json\", \"r\") as f:\n",
        "    test = pd.DataFrame(json.loads(f.read()))"
      ],
      "id": "a4da5d18"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VarLws341lg"
      },
      "outputs": [],
      "source": [
        "seed = 715\n",
        "num_samples = 10\n",
        "sample_MT = val_agnostic[val_agnostic['task'] == 'MT'].sample(num_samples, random_state=seed)\n",
        "sample_DM = val_agnostic[val_agnostic['task'] == 'DM'].sample(num_samples, random_state=seed)\n",
        "sample_PG = val_agnostic[val_agnostic['task'] == 'PG'].sample(num_samples, random_state=seed)"
      ],
      "id": "9VarLws341lg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b64bfa21"
      },
      "outputs": [],
      "source": [
        "sample_MT_results =set_label(sample_MT,prompt,num_samples)\n",
        "sample_MT_results.head()\n",
        "time.sleep(18)"
      ],
      "id": "b64bfa21"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIiN4bFTLqC4"
      },
      "outputs": [],
      "source": [
        "sample_MT_results['GPT_label'] = sample_MT_results['GPT_label'].astype(float) / 5"
      ],
      "id": "lIiN4bFTLqC4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76xoi5qbWz_o"
      },
      "outputs": [],
      "source": [
        "sample_DM_results =set_label(sample_DM,prompt,num_samples)\n",
        "sample_DM_results.head()\n",
        "time.sleep(18)"
      ],
      "id": "76xoi5qbWz_o"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7JUjc7EiQzY"
      },
      "outputs": [],
      "source": [
        "sample_DM_results['GPT_label'] = sample_DM_results['GPT_label'].astype(float) / 5"
      ],
      "id": "n7JUjc7EiQzY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "To9fo-YPXE0q"
      },
      "outputs": [],
      "source": [
        "sample_PG_results =set_label(sample_PG,prompt,num_samples)\n",
        "sample_PG_results.head()"
      ],
      "id": "To9fo-YPXE0q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpEYH8jlSEkm"
      },
      "outputs": [],
      "source": [
        "sample_PG_results['GPT_label'] = sample_PG_results['GPT_label'].astype(float) / 5"
      ],
      "id": "TpEYH8jlSEkm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJ4o_1o1972j"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "def eval_samples(sample_df):\n",
        "    # mean square error\n",
        "    def compute_mse(labels_array, preds_array):\n",
        "        return np.mean(np.square(labels_array - preds_array))\n",
        "\n",
        "    # mean absolute error\n",
        "    def compute_mae(labels_array, preds_array):\n",
        "        return np.mean(np.absolute(labels_array - preds_array))\n",
        "\n",
        "    # Spearman Correlation\n",
        "    def S_correlation(labels_array, preds_array):\n",
        "        spearman_corr, p_value = scipy.stats.spearmanr(labels_array, preds_array)\n",
        "        return spearman_corr\n",
        "\n",
        "    labels_array = sample_df['p(Hallucination)'].astype(float)\n",
        "    preds_array = sample_df['GPT_label'].astype(float)\n",
        "\n",
        "    print(\"MSE: \", compute_mse(labels_array, preds_array))\n",
        "    print(\"MAE: \", compute_mae(labels_array, preds_array))\n",
        "    print(\"Spearman Correlation: \", S_correlation(labels_array, preds_array))"
      ],
      "id": "dJ4o_1o1972j"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DYKNC6fZCx7a",
        "outputId": "42b9f541-ad30-4000-c400-22109a6cf5bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of samples: \n",
            "10\n",
            "MT:\n",
            "MSE:  0.168\n",
            "MAE:  0.27999999999999997\n",
            "Spearman Correlation:  0.24759378423606915\n",
            "*******\n",
            "DM:\n",
            "MSE:  0.328\n",
            "MAE:  0.48\n",
            "Spearman Correlation:  -0.18668216415808592\n",
            "*******\n",
            "PG:\n",
            "MSE:  0.05600000000000001\n",
            "MAE:  0.2\n",
            "Spearman Correlation:  0.6960784313725489\n",
            "*******\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of samples: \")\n",
        "print(num_samples)\n",
        "print(\"MT:\")\n",
        "eval_samples(sample_MT_results)\n",
        "print(\"*******\")\n",
        "print(\"DM:\")\n",
        "eval_samples(sample_DM_results)\n",
        "print(\"*******\")\n",
        "print(\"PG:\")\n",
        "eval_samples(sample_PG_results)\n",
        "print(\"*******\")"
      ],
      "id": "DYKNC6fZCx7a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6WeyyeVkOP4E",
        "outputId": "48c6c25b-dd08-4e95-cf09-bec1e0d111bb"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-25-5a9ce85ef757>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Number of samples:\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "Number of samples:\n",
        "10\n",
        "MT:\n",
        "MSE:  0.17200000000000001\n",
        "MAE:  0.29999999999999993\n",
        "Spearman Correlation:  -0.37139067635410367\n",
        "*******\n",
        "DM:\n",
        "MSE:  0.34800000000000003\n",
        "MAE:  0.54\n",
        "Spearman Correlation:  -0.2435116274707893\n",
        "*******\n",
        "PG:\n",
        "MSE:  0.15999999999999998\n",
        "MAE:  0.31999999999999995\n",
        "Spearman Correlation:  -0.006713829120923006\n",
        "*******"
      ],
      "id": "6WeyyeVkOP4E"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}