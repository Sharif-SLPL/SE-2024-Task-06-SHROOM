# -*- coding: utf-8 -*-
"""Mistral_Llama2_Zephyr_model_aware.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kAqWfifaQx-Pgo0jJgyC98Xg0kQxwyLT
"""

# Run this cell to mount your Google Drive.
from google.colab import drive
drive.mount('/content/drive')

import logging
logging.basicConfig(level='ERROR')

import pandas as pd

!pip install -q -U bitsandbytes
!pip install -q -U git+https://github.com/huggingface/transformers.git
!pip install -q -U git+https://github.com/huggingface/peft.git
!pip install -q -U git+https://github.com/huggingface/accelerate.git
!pip install -q -U einops
!pip install -q -U safetensors
!pip install -q -U torch
!pip install -q -U xformers
!pip install -q -U langchain
!pip install -q -U ctransformers[cuda]
!pip install chromadb
!pip install sentence-transformers

import torch
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)

model_id = "HuggingFaceH4/zephyr-7b-beta"

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

model_4bit = AutoModelForCausalLM.from_pretrained( model_id, device_map="auto",quantization_config=quantization_config, )

tokenizer = AutoTokenizer.from_pretrained(model_id)

pipeline = pipeline(
        "text-generation",
        model=model_4bit,
        tokenizer=tokenizer,
        use_cache=True,
        device_map="auto",
        max_length=5000,
        do_sample=True,
        top_k=5,
        num_return_sequences=1,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id,
)

from langchain import HuggingFacePipeline
from langchain import PromptTemplate, LLMChain
llm = HuggingFacePipeline(pipeline=pipeline)

template_general_4_1 = """<s>[INST] You are a helpful, respectful and honest assistant.
Answer the following question.
{question}
I asked two experts to determine whether the Sentence is supported by the Context or not.
Above are their explanations.Now judge which one gave a better reason.Give me just the index of the best expert with no explanations using this json format: {{index: (an integer number between 0 and 1, which 0 is for the first, 1 is for the second)}}.
 [/INST] </s>
"""

def chat_with_llm(question , template):
    prompt = PromptTemplate(template=template, input_variables=["question"])
    llm_chain = LLMChain(prompt=prompt, llm=llm)
    response = llm_chain.run({"question":question})
    return response

run_on_test = True # whether this baseline system is ran on the test splits or the val splits

import json
import regex

# simple JSON loading

path_mistral_model_aware = "/content/drive/MyDrive/sem_eval_task_6/for_zephyr_arbiter/test.model-aware-mistral-description.json"

with open(path_mistral_model_aware, 'r') as istr:
    mistral_data_model_aware = json.load(istr)

path_llama2_model_aware = "/content/drive/MyDrive/sem_eval_task_6/for_zephyr_arbiter/test.model-aware-llama2-description.json"

with open(path_llama2_model_aware, 'r') as istr:
    llama2_data_model_aware = json.load(istr)

path_test_model_aware = "/content/drive/MyDrive/sem_eval_task_6/test.model-aware.json"

with open(path_test_model_aware, 'r') as istr:
    data_val_all = json.load(istr)

mistral_data_model_aware[0]['description']

llama2_data_model_aware[0]['description']

data_val_all[0]

# change paths appropriately
# make sure the output filename is the same as the reference filename for the scoring program
##path_val_model_aware = "/content/SHROOM_test-unlabeled/test.model-aware.json"
path_judge_index_model_aware_output = "/content/drive/MyDrive/sem_eval_task_6/test.model-aware-final-judge_index_1.json"

#from datasets import load_dataset
import json
import random
import numpy as np
import tqdm.notebook as tqdm
seed_val = 442
random.seed(seed_val)
np.random.seed(seed_val)

# simple JSON loading
##with open(path_val_model_aware, 'r') as istr:
##    data_val_all = json.load(istr)
num_sample = len(data_val_all)
print(num_sample)
#num_sample = 5
output_json = []
labels = ["Not Hallucination", "Hallucination"]
"""
SelfCheckGPT Usage: (LLM) Prompt
https://github.com/potsawee/selfcheckgpt
Context: {}
Sentence: {}
Is the sentence supported by the context above?
Answer Yes or No:
"""
pattern = regex.compile(r'\{(?:[^{}]|(?R))*\}')

for i in tqdm.trange(num_sample):
    task = str(data_val_all[i]['task'])
    if run_on_test:
        # test splits will contain ids to ensure correct alignment before scoring
        id = int(data_val_all[i]['id'])
    hyp = str(data_val_all[i]['hyp'])
    src = str(data_val_all[i]['src'])
    tgt = str(data_val_all[i]['tgt'])

    if task == "PG":
        context = f"{src}"
    else: #i.e. task == "MT" or task == "DM":
        context = f"{tgt}"

    if str(mistral_data_model_aware[i]['label']) == 'Not Hallucination':
        mistral_label = 'yes'
    else:
        mistral_label = 'no'

    if str(llama2_data_model_aware[i]['label']) == 'Not Hallucination':
        llama2_label = 'yes'
    else:
        llama2_label = 'no'

    mistral_desc = str(mistral_data_model_aware[i]['description'])
    llama2_desc = str(llama2_data_model_aware[i]['description'])

    message = f"Context: {context}\nSentence: {hyp}\nFirst expert: {mistral_label} , {mistral_desc}\nSecond expert: {llama2_label} , {llama2_desc}"

    item_to_json=[]
    idx = 0
    for j in range(20):
      try:
        response = chat_with_llm(message, template_general_4_1)
        answer = str(response.strip().lower()).replace("```", "")
        match = pattern.findall(answer)[0]
        print(match)
        p = json.loads(match)
        item_to_json.append(p)
        idx = idx + 1
        if (idx == 1):
          break;
      except:
        print("An exception occurred")

    if run_on_test:
        item_to_json.append({"id":id})

    output_json.append(item_to_json)

    f = open(path_judge_index_model_aware_output, 'w', encoding='utf-8')
    json.dump(output_json, f)
    f.close()
print("done")