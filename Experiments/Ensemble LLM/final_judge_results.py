# -*- coding: utf-8 -*-
"""Final_judge_results.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MofF87P_fPWBLcGkjAwz_mRhQYudvJJ3
"""

# simple JSON loading

path_mistral_model_agnostic = "/home/slpl/results/mistral/test.model-agnostic-description.json"

with open(path_mistral_model_agnostic, 'r') as istr:
    mistral_data_model_agnostic = json.load(istr)

path_llama2_model_aware = "/home/slpl/results/arbiter_results/test.model-aware-llama2-description.json"

with open(path_llama2_model_aware, 'r') as istr:
    llama2_data_model_aware = json.load(istr)

path_llama2_model_agnostic = "/home/slpl/results/arbiter_results/test.model-agnostic-llama2-description.json"

with open(path_llama2_model_agnostic, 'r') as istr:
    llama2_data_model_agnostic = json.load(istr)

path_index_model_agnostic = "/home/slpl/results/arbiter_results/test.model-agnostic-final-judge.json"

with open(path_index_model_agnostic, 'r') as istr:
    index_data_model_agnostic = json.load(istr)

path_index_model_aware = "/home/slpl/results/arbiter_results/test.model-aware-final-judge.json"

with open(path_index_model_aware, 'r') as istr:
    index_data_model_aware = json.load(istr)

model_aware_final_output = []

for i in tqdm.trange(1500):
    try:
        if index_data_model_aware[i][1]['id'] == 0:
            model_aware_final_output.append(mistral_data_model_aware[i])
        else:
            model_aware_final_output.append(llama2_data_model_aware[i])

    except:
        task = str(data_val_all[i]['task'])
        if run_on_test:
            # test splits will contain ids to ensure correct alignment before scoring
            id = int(data_val_all[i]['id'])
        hyp = str(data_val_all[i]['hyp'])
        src = str(data_val_all[i]['src'])
        tgt = str(data_val_all[i]['tgt'])

        if task == "PG":
            context = f"{src}"
        else: #i.e. task == "MT" or task == "DM":
            context = f"{tgt}"

        if str(mistral_data_model_aware[i]['label']) == 'Not Hallucination':
            mistral_label = 'yes'
        else:
            mistral_label = 'no'

        if str(llama2_data_model_aware[i]['label']) == 'Not Hallucination':
            llama2_label = 'yes'
        else:
            llama2_label = 'no'

        mistral_desc = str(mistral_data_model_aware[i]['description'])
        llama2_desc = str(llama2_data_model_aware[i]['description'])

        message = f"Context: {context}\nSentence: {hyp}\nFirst expert: {mistral_label} , {mistral_desc}\nSecond expert: {llama2_label} , {llama2_desc}"

        idx = 0
        for j in range(30):
          try:
            response = chat_with_llm(message, template_general_4_1)
            answer = str(response.strip().lower()).replace("```", "")
            match = pattern.findall(answer)[0]
            print(match)
            p = json.loads(match)
            idx = idx + 1
            if (idx == 1):
              break;
          except:
            print("An exception occurred")

        if p['index'] == 0:
            model_aware_final_output.append(mistral_data_model_aware[i])
        else:
            model_aware_final_output.append(llama2_data_model_aware[i])

path_judge_real_model_aware_output = "/home/slpl/results/arbiter_results/test.model-aware-final-judge_real.json"
f = open(path_judge_real_model_aware_output, 'w', encoding='utf-8')
json.dump(model_aware_final_output, f)
f.close()

model_agnostic_final_output = []

for i in tqdm.trange(1500):
    try:
        if index_data_model_agnostic[i][1]['id'] == 0:
            model_agnostic_final_output.append(mistral_data_model_agnostic[i])
        else:
            model_agnostic_final_output.append(llama2_data_model_agnostic[i])

    except:
        task = str(data_val_all[i]['task'])
        if run_on_test:
            # test splits will contain ids to ensure correct alignment before scoring
            id = int(data_val_all[i]['id'])
        hyp = str(data_val_all[i]['hyp'])
        src = str(data_val_all[i]['src'])
        tgt = str(data_val_all[i]['tgt'])

        if task == "PG":
            context = f"{src}"
        else: #i.e. task == "MT" or task == "DM":
            context = f"{tgt}"

        if str(mistral_data_model_agnostic[i]['label']) == 'Not Hallucination':
            mistral_label = 'yes'
        else:
            mistral_label = 'no'

        if str(llama2_data_model_agnostic[i]['label']) == 'Not Hallucination':
            llama2_label = 'yes'
        else:
            llama2_label = 'no'

        mistral_desc = str(mistral_data_model_agnostic[i]['description'])
        llama2_desc = str(llama2_data_model_agnostic[i]['description'])

        message = f"Context: {context}\nSentence: {hyp}\nFirst expert: {mistral_label} , {mistral_desc}\nSecond expert: {llama2_label} , {llama2_desc}"

        idx = 0
        for j in range(30):
          try:
            response = chat_with_llm(message, template_general_4_1)
            answer = str(response.strip().lower()).replace("```", "")
            match = pattern.findall(answer)[0]
            print(match)
            p = json.loads(match)
            idx = idx + 1
            if (idx == 1):
              break;
          except:
            print("An exception occurred")

        if p['index'] == 0:
            model_agnostic_final_output.append(mistral_data_model_agnostic[i])
        else:
            model_agnostic_final_output.append(llama2_data_model_agnostic[i])

path_judge_real_model_agnostic_output = "/home/slpl/results/arbiter_results/test.model-agnostic-final-judge_real.json"
if len(model_agnostic_final_output) == 1500:
    f = open(path_judge_real_model_agnostic_output, 'w', encoding='utf-8')
    json.dump(model_agnostic_final_output, f)
    f.close()