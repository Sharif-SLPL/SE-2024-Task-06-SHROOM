# -*- coding: utf-8 -*-
"""Mistral_Arbiter_Final_model_agnostic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1alcYBdZp6nc_OGyTDq5BrZeFtNOwJqSk
"""

# Run this cell to mount your Google Drive.
from google.colab import drive
drive.mount('/content/drive')

import logging
logging.basicConfig(level='ERROR')

import pandas as pd

!pip install -q -U bitsandbytes
!pip install -q -U git+https://github.com/huggingface/transformers.git
!pip install -q -U git+https://github.com/huggingface/peft.git
!pip install -q -U git+https://github.com/huggingface/accelerate.git
!pip install -q -U einops
!pip install -q -U safetensors
!pip install -q -U torch
!pip install -q -U xformers
!pip install -q -U langchain
!pip install -q -U ctransformers[cuda]
!pip install chromadb
!pip install sentence-transformers

import torch
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
)

model_id = "mistralai/Mistral-7B-Instruct-v0.1"

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

model_4bit = AutoModelForCausalLM.from_pretrained( model_id, device_map="auto",quantization_config=quantization_config, )

tokenizer = AutoTokenizer.from_pretrained(model_id)

pipeline = pipeline(
        "text-generation",
        model=model_4bit,
        tokenizer=tokenizer,
        use_cache=True,
        device_map="auto",
        max_length=5000,
        do_sample=True,
        top_k=5,
        num_return_sequences=1,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id,
)

from langchain import HuggingFacePipeline
from langchain import PromptTemplate, LLMChain
llm = HuggingFacePipeline(pipeline=pipeline)

template_general_9 = """<s>[INST] You are a helpful, respectful and honest assistant.
Two experts are asked whether the given sentence supports the given context or not. We received two responses from these two experts. According to the explanations of these two experts, what is your decision? return your response in this json format {{label: (yes/no), score: (an integer number between 1 and 5, which 1 is for not supported and 5 is for full supported), explanation: (text)}}.
{question} [/INST] </s>
"""

def chat_with_llm(question , template):
    prompt = PromptTemplate(template=template, input_variables=["question"])
    llm_chain = LLMChain(prompt=prompt, llm=llm)
    response = llm_chain.run({"question":question})
    return response

run_on_test = True # whether this baseline system is ran on the test splits or the val splits

import json
import regex

# simple JSON loading

path_mistral_model_agnostic = "/home/slpl/results/test.model-aware-1500.json"

with open(path_mistral_model_agnostic, 'r') as istr:
    mistral_data_model_agnostic = json.load(istr)

path_zephyr_model_agnostic = "/home/slpl/results/test.model-aware-1500.json"

with open(path_zephyr_model_agnostic, 'r') as istr:
    zephyr_data_model_agnostic = json.load(istr)

path_test_model_agnostic = "/home/slpl/SHROOM_test-unlabeled/test.model-aware.json"

with open(path_test_model_agnostic, 'r') as istr:
    data_val_all = json.load(istr)

mistral_data_model_agnostic[0]['description']

zephyr_data_model_agnostic[0]['zephyr_description']

data_val_all[0]

# change paths appropriately
# make sure the output filename is the same as the reference filename for the scoring program
##path_val_model_aware = "/content/SHROOM_test-unlabeled/test.model-aware.json"
path_val_model_agnostic_output = "/home/slpl/results/test.model-aware-final-arbiter.json"

#from datasets import load_dataset
import json
import random
import numpy as np
import tqdm.notebook as tqdm
seed_val = 442
random.seed(seed_val)
np.random.seed(seed_val)

# simple JSON loading
##with open(path_val_model_aware, 'r') as istr:
##    data_val_all = json.load(istr)
num_sample = len(data_val_all)
print(num_sample)
#num_sample = 5
output_json = []
labels = ["Not Hallucination", "Hallucination"]
"""
SelfCheckGPT Usage: (LLM) Prompt
https://github.com/potsawee/selfcheckgpt
Context: {}
Sentence: {}
Is the sentence supported by the context above?
Answer Yes or No:
"""
pattern = regex.compile(r'\{(?:[^{}]|(?R))*\}')

for i in tqdm.trange(num_sample):
    task = str(data_val_all[i]['task'])
    if run_on_test:
        # test splits will contain ids to ensure correct alignment before scoring
        id = int(data_val_all[i]['id'])
    hyp = str(data_val_all[i]['hyp'])
    src = str(data_val_all[i]['src'])
    tgt = str(data_val_all[i]['tgt'])

    if task == "PG":
        context = f"{src}"
    else: #i.e. task == "MT" or task == "DM":
        context = f"{tgt}"

    zephyr_label = str(zephyr_data_model_agnostic[i]['zephyr_label'])
    if str(mistral_data_model_agnostic[i]['label']) == 'Not Hallucination':
        mistral_label = 'yes'
    else:
        mistral_label = 'no'

    mistral_desc = str(mistral_data_model_agnostic[i]['description'])
    zephyr_desc = str(zephyr_data_model_agnostic[i]['zephyr_description'])

    message = f"Context: {context}\nSentence: {hyp}\nFirst expert: {mistral_label} , {mistral_desc}\nSecond expert: {zephyr_label} , {zephyr_desc}"

    item_to_json=[]
    idx = 0
    for j in range(10):
      try:
        response = chat_with_llm(message, template_general_9)
        answer = str(response.strip().lower()).replace("```", "")
        match = pattern.findall(answer)[0]
        print(match)
        p = json.loads(match)
        item_to_json.append(p)
        idx = idx + 1
        if (idx == 1):
          break;
      except:
        print("An exception occurred")

    if run_on_test:
        item_to_json.append({"id":id})

    output_json.append(item_to_json)

    f = open(path_val_model_agnostic_output, 'w', encoding='utf-8')
    json.dump(output_json, f)
    f.close()
print("done")